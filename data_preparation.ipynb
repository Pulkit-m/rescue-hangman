{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "* This file contains utilities like: \n",
    "    - Train test split\n",
    "    - dataset preparation for training Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Last Script Run: Tue Mar 12 15:57:43 2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os, sys, time\n",
    "import random\n",
    "print(\"Time Last Script Run: \" + time.asctime())\n",
    "\n",
    "import string \n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "ascii_lowercase = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227300 words found\n"
     ]
    }
   ],
   "source": [
    "# reading all words from the dictionary. \n",
    "dictionaryFile = open(\"./data/raw/words_250000_train.txt\",\"r\") \n",
    "wordList = list(map(lambda x : x.strip() ,dictionaryFile.readlines()))\n",
    "lenDict = len(wordList) \n",
    "dictionaryFile.close()\n",
    "print(str(lenDict) + \" words found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train and test folders to segregate data for training and validation. \n",
    "os.makedirs(\"./data/train\", exist_ok=True) \n",
    "os.makedirs(\"./data/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training and testing splits: \n",
      "Train: 181840 \n",
      "Test: 45460\n",
      "Saving train and test files to ./data/train/train.txt & ./data/test/test.txt respectively\n"
     ]
    }
   ],
   "source": [
    "# shuffling data from the source file.\n",
    "random.shuffle(wordList)\n",
    "train_size = int(0.8*lenDict)\n",
    "trainWordList = wordList[:train_size]\n",
    "testWordList = wordList[train_size:]\n",
    "\n",
    "# saving files: \n",
    "train_path = \"./data/train/train.txt\" \n",
    "with open(train_path, 'w') as f: \n",
    "    for word in trainWordList: \n",
    "        f.write(word + '\\n') \n",
    "test_path = \"./data/test/test.txt\" \n",
    "with open(test_path, 'w') as f: \n",
    "    for word in testWordList: \n",
    "        f.write(word + '\\n') \n",
    "            \n",
    "print(f\"creating training and testing splits: \\nTrain: {len(trainWordList)} \\nTest: {len(testWordList)}\")\n",
    "print(f\"Saving train and test files to {train_path} & {test_path} respectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Modelling using three inputs: \n",
    "1. Input padded and one hot encoded word with blank spaces as '_'.\n",
    "2. Input available characters from the english alphabet that are still availabe for gressing. \n",
    "3. Input wrong characters from the english alphabet that should not be used. \n",
    "\n",
    "Target: correctly missing characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181840/181840 [00:58<00:00, 3121.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File from storage\n",
      "Shuffling data\n",
      "Saving data as a csv file.\n"
     ]
    }
   ],
   "source": [
    "def allUniqCombinations(word, drop_combinations = 0):\n",
    "    uniq_chars = np.unique(list(word)).tolist()\n",
    "    num_uniq_chars = len(uniq_chars)\n",
    "    uniq_combos = []\n",
    "    for r in range(1,num_uniq_chars+1):\n",
    "        combinations = list(itertools.combinations(uniq_chars,r)) \n",
    "        uniq_combos = uniq_combos + combinations\n",
    "\n",
    "    N = len(uniq_combos) \n",
    "    drop = int(N*drop_combinations)\n",
    "    combos = random.sample(uniq_combos,k=N-drop) \n",
    "    return combos\n",
    "\n",
    "\n",
    "def wordDecay(word,chars_to_remove,num_wrong_guesses = 6): \n",
    "    original_word = word\n",
    "    for ch in chars_to_remove: \n",
    "        word = word.replace(ch,'_') \n",
    "\n",
    "    alphabet = string.ascii_lowercase\n",
    "    for ch in list(word):\n",
    "        alphabet = alphabet.replace(ch,'')  \n",
    "\n",
    "    num_wrong_guesses = random.randint(0,min(num_wrong_guesses,len(alphabet)//3))\n",
    "    wrong_guesses = ''.join(list(filter(                            \n",
    "                    lambda x : x not in chars_to_remove,            \n",
    "                    random.sample(alphabet,k=num_wrong_guesses))))  \n",
    "\n",
    "    for ch in list(wrong_guesses): \n",
    "        alphabet = alphabet.replace(ch,'')\n",
    "\n",
    "    # word, chars_to_remove, alphabet, wrong_guesses\n",
    "    return f\"{original_word},{word},{''.join(chars_to_remove)},{alphabet},{wrong_guesses}\"\n",
    "\n",
    "\n",
    "def prepareDataset(source_path,save_path, drop_combinations = 0):\n",
    "    with open(source_path,'r') as s: \n",
    "        wordList = list(map(lambda x : x.strip(), s.readlines()))\n",
    "    \n",
    "    file = open(save_path,'w') \n",
    "    for word in tqdm(wordList):\n",
    "        uniq_removal_combinations = allUniqCombinations(word, drop_combinations) \n",
    "        dataset_word = []\n",
    "        for combo in uniq_removal_combinations:\n",
    "            dataset_word.append(wordDecay(word,combo)) \n",
    "\n",
    "        to_write = '\\n'.join(dataset_word) \n",
    "        file.write(to_write) \n",
    "        file.write('\\n')\n",
    "\n",
    "    file.close()\n",
    "    print(\"Loading File from storage\") \n",
    "    df = pd.read_csv(save_path,names=[\"word\",\"input\",\"target\",\"available\",\"missed\"])\n",
    "    print(f\"{df.shape[0]} records found in training dataset\")\n",
    "    print(\"Shuffling data\")\n",
    "    df = df.sample(frac=1) \n",
    "    df.to_csv(save_path[:-4] + \".csv\",index=False)\n",
    "    print(\"Saving data as a csv file.\") \n",
    "\n",
    "\n",
    "if __name__==\"__main__\": \n",
    "    drop_data_ratio = 0.95  # to reduce size of training data  \n",
    "    source_data_path = \"./data/train/train.txt\" \n",
    "    save_path = \"./data/train/train_io_new.txt\"         # pass in a txt extension, will save both csv and txt\n",
    "    prepareDataset(source_path=source_data_path,save_path=save_path, drop_combinations=drop_data_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
