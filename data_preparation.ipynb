{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Last Script Run: Sun Mar 10 12:00:15 2024\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os, sys, time\n",
    "import random\n",
    "print(\"Time Last Script Run: \" + time.asctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227300 words found\n"
     ]
    }
   ],
   "source": [
    "# reading all words from the dictionary. \n",
    "dictionaryFile = open(\"./data/raw/words_250000_train.txt\",\"r\") \n",
    "wordList = list(map(lambda x : x.strip() ,dictionaryFile.readlines()))\n",
    "lenDict = len(wordList) \n",
    "dictionaryFile.close()\n",
    "print(str(lenDict) + \" words found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train and test folders to segregate data for training and validation. \n",
    "os.makedirs(\"./data/train\", exist_ok=True) \n",
    "os.makedirs(\"./data/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating training and testing splits: \n",
      "Train: 181840 \n",
      "Test: 45460\n",
      "Saving train and test files to ./data/train/train.txt & ./data/test/test.txt respectively\n"
     ]
    }
   ],
   "source": [
    "# shuffling data from the source file.\n",
    "random.shuffle(wordList)\n",
    "train_size = int(0.8*lenDict)\n",
    "trainWordList = wordList[:train_size]\n",
    "testWordList = wordList[train_size:]\n",
    "\n",
    "# saving files: \n",
    "train_path = \"./data/train/train.txt\" \n",
    "with open(train_path, 'w') as f: \n",
    "    for word in trainWordList: \n",
    "        f.write(word + '\\n') \n",
    "test_path = \"./data/test/test.txt\" \n",
    "with open(test_path, 'w') as f: \n",
    "    for word in testWordList: \n",
    "        f.write(word + '\\n') \n",
    "            \n",
    "print(f\"creating training and testing splits: \\nTrain: {len(trainWordList)} \\nTest: {len(testWordList)}\")\n",
    "print(f\"Saving train and test files to {train_path} & {test_path} respectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import itertools\n",
    "# import torch\n",
    "# import torch.nn.functional as Fun\n",
    "from tqdm import tqdm\n",
    "ascii_lowercase = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allUniqCombinations(word, drop_combinations = 0):\n",
    "    uniq_chars = np.unique(list(word)).tolist()\n",
    "    num_uniq_chars = len(uniq_chars)\n",
    "    uniq_combos = []\n",
    "    for r in range(1,num_uniq_chars+1):\n",
    "        combinations = list(itertools.combinations(uniq_chars,r)) \n",
    "        uniq_combos = uniq_combos + combinations\n",
    "\n",
    "    N = len(uniq_combos) \n",
    "    drop = int(N*drop_combinations)\n",
    "    combos = random.sample(uniq_combos,k=N-drop) \n",
    "    return combos\n",
    "\n",
    "def wordDecay(word,chars_to_remove): \n",
    "    original_word = word\n",
    "    for ch in chars_to_remove: \n",
    "        word = word.replace(ch,'_') \n",
    "\n",
    "    alphabet = string.ascii_lowercase\n",
    "    for ch in list(word):\n",
    "        alphabet = alphabet.replace(ch,'') \n",
    "    \n",
    "    return f\"{original_word},{word},{''.join(chars_to_remove)},{alphabet}\"\n",
    "\n",
    "\n",
    "def prepareDataset(source_path,save_path, drop_combinations = 0):\n",
    "    with open(source_path,'r') as s: \n",
    "        wordList = list(map(lambda x : x.strip(), s.readlines()))\n",
    "    \n",
    "    file = open(save_path,'w') \n",
    "    for word in tqdm(wordList):\n",
    "        uniq_removal_combinations = allUniqCombinations(word, drop_combinations) \n",
    "        dataset_word = []\n",
    "        for combo in uniq_removal_combinations:\n",
    "            dataset_word.append(wordDecay(word,combo)) \n",
    "\n",
    "        to_write = '\\n'.join(dataset_word) \n",
    "        file.write(to_write) \n",
    "        file.write('\\n')\n",
    "\n",
    "    file.close()\n",
    "    print(\"Loading File from storage\") \n",
    "    df = pd.read_csv(save_path,names=[\"word\",\"input\",\"target\",\"available\"])\n",
    "    print(\"Shuffling data\")\n",
    "    df = df.sample(frac=1) \n",
    "    df.to_csv(save_path[:-4] + \".csv\",index=False)\n",
    "    print(\"Saving data as a csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181840/181840 [00:31<00:00, 5733.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading File from storage\n",
      "Shuffling data\n",
      "Saving data as a csv file.\n"
     ]
    }
   ],
   "source": [
    "prepareDataset(source_path=\"./data/train/train.txt\",save_path=\"./data/train/train_io.txt\", drop_combinations=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
